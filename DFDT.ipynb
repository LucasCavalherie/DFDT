{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/LucasCavalherie/DFDT/blob/main/DFDT.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# DFDT: (Dynamic Fast Decision Tree)\n",
        "\n",
        "Esse código foi inspirado em um pseudocódigo apresentado no artigo:\n",
        "\n",
        "DFDT: Dynamic Fast Decision Tree for IoT Data Stream Mining on Edge Devices (https://arxiv.org/pdf/2502.14011)\n"
      ],
      "metadata": {
        "id": "IJ9nTsF5ilR3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "### **1. Imports**\n",
        "\n"
      ],
      "metadata": {
        "id": "NKiLIakgix7N"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. Imports\n",
        "import math\n",
        "import random\n",
        "import csv\n",
        "import time\n",
        "import os\n",
        "import pandas as pd\n",
        "import itertools\n",
        "from collections import defaultdict"
      ],
      "metadata": {
        "id": "__QgVBNLjO4y"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "### **2. Componentes Principais**\n",
        "Blocos de componentes fundamentais e as funções auxiliares exigidas pelo algoritmo principal.\n",
        "\n"
      ],
      "metadata": {
        "id": "rXEhRTlbiyy2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **Calculadora de Gini Impurity**\n",
        "Esta é uma função auxiliar para medir a impureza de um conjunto de pontos de dados.\n",
        "\n",
        "* **O que é:** `Gini Impurity` é uma métrica que varia de 0 a 0.5. Um valor de 0 significa que todos os pontos de dados em um nó pertencem à mesma `class` (perfeitamente puro). Um valor de 0.5 significa que as classes estão perfeitamente misturadas (impureza máxima para um caso binário).\n",
        "* **Como funciona:** Calcula a probabilidade de classificar incorretamente um elemento escolhido aleatoriamente se ele fosse rotulado aleatoriamente de acordo com a distribuição de rótulos no subconjunto.\n",
        "* **Parâmetros:**\n",
        "  * `class_distribution`: Um dicionário que mapeia os rótulos de `class` para suas contagens (ex: `{'rain': 10, 'no_rain': 20}`)."
      ],
      "metadata": {
        "id": "H4QTvqnnjCjf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 2. Core Components\n",
        "def calculate_gini_impurity(class_distribution):\n",
        "    \"\"\"Calculates the Gini Impurity for a class distribution.\"\"\"\n",
        "    total_count = sum(class_distribution.values())\n",
        "    if total_count == 0:\n",
        "        return 0.0\n",
        "\n",
        "    impurity = 1.0\n",
        "    for count in class_distribution.values():\n",
        "        probability = count / total_count\n",
        "        impurity -= probability ** 2\n",
        "    return impurity"
      ],
      "metadata": {
        "id": "b0iD-hKzjRxq"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **Numeric Attribute Splitter**\n",
        "Esta classe é o mecanismo de aprendizagem principal para um único atributo numérico. Seu trabalho é encontrar o melhor ponto de divisão possível para os dados que observou.\n",
        "\n",
        "* **O que é:** Um \"estatístico\" dedicado para uma `feature` (ex: 'temperature'). Ele mantém o registo de todos os valores e rótulos de `class` correspondentes que viu para essa `feature`.\n",
        "* **Como funciona:** O método `get_best_split` itera sobre todos os valores únicos observados, testando o ponto médio entre cada par como um limiar de divisão potencial. Para cada candidato, ele calcula o `Gini Gain` (a redução na impureza) e retorna o ponto de divisão que fornece o maior ganho.\n",
        "* **Métodos Chave:**\n",
        "  * `update(value, class_label)`: Adiciona um novo ponto de dados ao seu armazenamento interno.\n",
        "  * `get_best_split(parent_impurity)`: Realiza a busca pelo melhor limiar de divisão."
      ],
      "metadata": {
        "id": "RBZAL60vizw-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class NumericAttributeSplitter:\n",
        "    \"\"\"\n",
        "    This class stores statistics for a single numeric attribute\n",
        "    and finds the best split point for it.\n",
        "    \"\"\"\n",
        "    def __init__(self):\n",
        "        self._value_class_observations = defaultdict(list)\n",
        "\n",
        "    def update(self, value, class_label):\n",
        "        \"\"\"Updates the splitter with a new value and its class.\"\"\"\n",
        "        self._value_class_observations[value].append(class_label)\n",
        "\n",
        "    def get_best_split(self, parent_impurity):\n",
        "        \"\"\"\n",
        "        Finds the best split point for this attribute.\n",
        "        It tests all midpoints between unique values as split candidates.\n",
        "        \"\"\"\n",
        "        best_split_value = None\n",
        "        best_gain = -1.0\n",
        "\n",
        "        sorted_unique_values = sorted(self._value_class_observations.keys())\n",
        "\n",
        "        if len(sorted_unique_values) < 2:\n",
        "            return None, -1.0\n",
        "\n",
        "        for i in range(len(sorted_unique_values) - 1):\n",
        "            split_candidate = (sorted_unique_values[i] + sorted_unique_values[i+1]) / 2\n",
        "\n",
        "            left_branch_dist = defaultdict(int)\n",
        "            right_branch_dist = defaultdict(int)\n",
        "\n",
        "            for value, classes in self._value_class_observations.items():\n",
        "                for c in classes:\n",
        "                    if value <= split_candidate:\n",
        "                        left_branch_dist[c] += 1\n",
        "                    else:\n",
        "                        right_branch_dist[c] += 1\n",
        "\n",
        "            left_count = sum(left_branch_dist.values())\n",
        "            right_count = sum(right_branch_dist.values())\n",
        "            total_count = left_count + right_count\n",
        "\n",
        "            if left_count == 0 or right_count == 0:\n",
        "                continue\n",
        "\n",
        "            left_impurity = calculate_gini_impurity(left_branch_dist)\n",
        "            right_impurity = calculate_gini_impurity(right_branch_dist)\n",
        "\n",
        "            weighted_children_impurity = (left_count / total_count) * left_impurity + \\\n",
        "                                         (right_count / total_count) * right_impurity\n",
        "\n",
        "            gain = parent_impurity - weighted_children_impurity\n",
        "\n",
        "            if gain > best_gain:\n",
        "                best_gain = gain\n",
        "                best_split_value = split_candidate\n",
        "\n",
        "        return best_split_value, best_gain"
      ],
      "metadata": {
        "id": "0vP1NH0QjTvl"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **Classe Node**\n",
        "Esta classe define a estrutura de cada nó na árvore de decisão. Um nó pode ser uma folha (fazendo predições) ou um nó de divisão (direcionando o tráfego).\n",
        "\n",
        "* **O que é:** A estrutura de dados fundamental da árvore.\n",
        "* **Atributos Chave:**\n",
        "  * `is_leaf`: Um booleano que indica se é um nó terminal.\n",
        "  * `class_distribution`: Um dicionário que armazena as contagens de classes que terminaram neste nó.\n",
        "  * `feature_estimators`: Um dicionário onde cada chave é um índice de atributo e cada valor é uma instância de `NumericAttributeSplitter`.\n",
        "  * `split_attribute` / `split_value`: Se o nó não for uma folha, esses atributos armazenam a regra para a divisão (ex: \"dividir no atributo 5 onde o valor <= 0.15\").\n",
        "  * Atributos específicos do DFDT (`n_l`, `n_check_l`, `active`, etc.) para gerir a lógica adaptativa."
      ],
      "metadata": {
        "id": "PlwICgPqi7a4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Node:\n",
        "    \"\"\"Represents a node in the decision tree with learning capabilities.\"\"\"\n",
        "    def __init__(self, is_leaf=True, parent=None, depth=0):\n",
        "        self.is_leaf = is_leaf\n",
        "        self.parent = parent\n",
        "        self.depth = depth\n",
        "        self.children = {}\n",
        "        self.active = True\n",
        "        self.id = random.randint(0, 1000000)\n",
        "\n",
        "        self.class_distribution = defaultdict(int)\n",
        "        self.feature_estimators = defaultdict(NumericAttributeSplitter)\n",
        "\n",
        "        self.n_l = 0\n",
        "        self.n_check_l = 0\n",
        "        self.n_leaf_l = 0\n",
        "        self.n_tree_l = 0\n",
        "\n",
        "        self.split_attribute = None\n",
        "        self.split_value = None\n",
        "\n",
        "    def __repr__(self):\n",
        "        return f\"Node(id={self.id}, is_leaf={self.is_leaf}, active={self.active}, n_l={self.n_l}, dist={self.class_distribution})\"\n",
        "\n",
        "    def get_prediction(self):\n",
        "        \"\"\"Gets the prediction from a leaf node (the majority class).\"\"\"\n",
        "        if not self.class_distribution:\n",
        "            return None\n",
        "        return max(self.class_distribution, key=self.class_distribution.get)\n",
        "\n",
        "    def update_stats(self, instance_X, instance_y):\n",
        "        \"\"\"Updates the leaf's statistics with a new instance.\"\"\"\n",
        "        self.class_distribution[instance_y] += 1\n",
        "        for attr_index, value in instance_X.items():\n",
        "            self.feature_estimators[attr_index].update(value, instance_y)"
      ],
      "metadata": {
        "id": "XcN-KyIvjWHn"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "---\n",
        "### **3. Classe Principal do Algoritmo DFDT**\n",
        "Esta classe encapsula todo o algoritmo DFDT, incluindo o seu estado (a árvore, estatísticas históricas) e toda a sua lógica para treino e predição.\n",
        "\n",
        "* **O que é:** O controlador principal da árvore de decisão.\n",
        "* **Como funciona:**\n",
        "  1. O método `__init__` inicializa uma árvore vazia com apenas um nó raiz e configura os hiperparâmetros.\n",
        "  2. O método `train` orquestra o processo de aprendizagem. Ele itera através do fluxo de dados, realizando a avaliação **test-then-train** para cada instância.\n",
        "  3. O método `predict` encaminha uma nova instância pela árvore para obter uma predição de `class`.\n",
        "  4. Métodos auxiliares internos (prefixados com `_`) lidam com a lógica complexa descrita no artigo do DFDT, como tentar uma divisão (`_attempt_growth`), verificar as condições de divisão (`_can_split`), realizar a divisão (`_split_leaf`) e adaptar o `grace period` (`_adapt_grace_period`).\n",
        "* **Métodos Públicos Chave:**\n",
        "  * `train(data_stream)`: O ponto de entrada principal para treinar o modelo.\n",
        "  * `predict(instance_X)`: Usado para obter uma predição para uma instância nova e não vista."
      ],
      "metadata": {
        "id": "IqLQcAKqjF1X"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 3. Main DFDT Algorithm Class\n",
        "class DFDT:\n",
        "    \"\"\"\n",
        "    An implementation of the Dynamic Fast Decision Tree (DFDT) algorithm\n",
        "    for streaming data, with expanded hyperparameters.\n",
        "    \"\"\"\n",
        "    def __init__(self, delta=0.05, initial_grace_period=100,\n",
        "                 grow_fast_threshold=2.0, deactivate_threshold=0.2,\n",
        "                 std_dev_multiplier_c3_c4=1.0, std_dev_multiplier_c5=1.0,\n",
        "                 r_heuristic_range=0.5):\n",
        "        self.delta = delta\n",
        "        self.initial_grace_period = initial_grace_period\n",
        "        self.grow_fast_threshold = grow_fast_threshold\n",
        "        self.deactivate_threshold = deactivate_threshold\n",
        "        self.std_dev_multiplier_c3_c4 = std_dev_multiplier_c3_c4\n",
        "        self.std_dev_multiplier_c5 = std_dev_multiplier_c5\n",
        "        self.R_HEURISTIC_RANGE = r_heuristic_range\n",
        "\n",
        "        self.root = Node(is_leaf=True, depth=0)\n",
        "        self.root.n_min_grace_period = self.initial_grace_period\n",
        "        self.leaves = {self.root}\n",
        "\n",
        "        # Historical statistics\n",
        "        self.h_stat_values = []\n",
        "        self.g_stat_values = []\n",
        "        self.n_stat_values = []\n",
        "        self.hb_stat_values = []\n",
        "\n",
        "    def _avg(self, values_list):\n",
        "        return sum(values_list) / len(values_list) if values_list else 0\n",
        "\n",
        "    def _std_dev(self, values_list):\n",
        "        if not values_list or len(values_list) < 2:\n",
        "            return 0\n",
        "        mean = self._avg(values_list)\n",
        "        variance = sum([(x - mean) ** 2 for x in values_list]) / (len(values_list) - 1)\n",
        "        return math.sqrt(variance)\n",
        "\n",
        "    def _route_to_leaf(self, instance_X):\n",
        "        current_node = self.root\n",
        "        while not current_node.is_leaf:\n",
        "            if current_node.split_attribute is None or current_node.split_value is None:\n",
        "                break\n",
        "            instance_value = instance_X.get(current_node.split_attribute, 0)\n",
        "            branch_index = 0 if instance_value <= current_node.split_value else 1\n",
        "            if branch_index in current_node.children:\n",
        "                current_node = current_node.children[branch_index]\n",
        "            else:\n",
        "                break\n",
        "        return current_node\n",
        "\n",
        "    def predict(self, instance_X):\n",
        "        leaf = self._route_to_leaf(instance_X)\n",
        "        return leaf.get_prediction()\n",
        "\n",
        "    def train(self, data_stream):\n",
        "        n_total_instances = 0\n",
        "        correct_predictions = 0\n",
        "        for i, (instance_X, instance_y) in enumerate(data_stream):\n",
        "            prediction = self.predict(instance_X)\n",
        "            if prediction is not None and prediction == instance_y:\n",
        "                correct_predictions += 1\n",
        "\n",
        "            leaf = self._route_to_leaf(instance_X)\n",
        "            leaf.update_stats(instance_X, instance_y)\n",
        "            n_total_instances += 1\n",
        "            leaf.n_l += 1\n",
        "            self._attempt_growth(leaf, n_total_instances)\n",
        "\n",
        "        return (correct_predictions / n_total_instances) * 100 if n_total_instances > 0 else 0\n",
        "\n",
        "    def _attempt_growth(self, leaf, n_total_instances):\n",
        "        if not leaf.active:\n",
        "            return\n",
        "\n",
        "        if n_total_instances - leaf.n_tree_l > 0:\n",
        "            fraction = ((leaf.n_l - leaf.n_leaf_l) * len(self.leaves)) / (n_total_instances - leaf.n_tree_l)\n",
        "        else:\n",
        "            fraction = 0\n",
        "\n",
        "        grow_fast_flag = fraction > self.grow_fast_threshold\n",
        "        if fraction < self.deactivate_threshold:\n",
        "            leaf.active = False\n",
        "            return\n",
        "\n",
        "        if not hasattr(leaf, 'n_min_grace_period'):\n",
        "            leaf.n_min_grace_period = self.initial_grace_period\n",
        "\n",
        "        is_ready_for_check = (leaf.n_l - leaf.n_check_l > leaf.n_min_grace_period)\n",
        "\n",
        "        if len(leaf.class_distribution) > 1 and is_ready_for_check:\n",
        "            epsilon = self._calculate_hoeffding_bound(leaf.n_l)\n",
        "            g_values = self._calculate_g_values(leaf)\n",
        "\n",
        "            if self._can_split(leaf, grow_fast_flag, g_values, epsilon):\n",
        "                self._split_leaf(leaf)\n",
        "            else:\n",
        "                leaf.n_check_l = leaf.n_l\n",
        "                self._adapt_grace_period(leaf, g_values, epsilon)\n",
        "\n",
        "    def _split_leaf(self, leaf):\n",
        "        leaf.is_leaf = False\n",
        "        leaf.active = False\n",
        "        self.leaves.remove(leaf)\n",
        "\n",
        "        split_attr, split_val = leaf.best_split_info['attribute'], leaf.best_split_info['split_value']\n",
        "        leaf.split_attribute = split_attr\n",
        "        leaf.split_value = split_val\n",
        "\n",
        "        left_child = Node(is_leaf=True, parent=leaf, depth=leaf.depth + 1)\n",
        "        right_child = Node(is_leaf=True, parent=leaf, depth=leaf.depth + 1)\n",
        "\n",
        "        for attr, splitter in leaf.feature_estimators.items():\n",
        "            for val, classes in splitter._value_class_observations.items():\n",
        "                for cls in classes:\n",
        "                    target_child = left_child if val <= split_val else right_child\n",
        "                    target_child.update_stats({attr: val}, cls)\n",
        "\n",
        "        self.leaves.add(left_child)\n",
        "        self.leaves.add(right_child)\n",
        "        leaf.children[0] = left_child\n",
        "        leaf.children[1] = right_child\n",
        "\n",
        "    def _can_split(self, leaf, grow_fast, g_values, epsilon):\n",
        "        if not g_values: return False\n",
        "        sorted_g = sorted(g_values.items(), key=lambda item: item[1], reverse=True)\n",
        "        if not sorted_g: return False\n",
        "\n",
        "        g_best_attr, g_best_val = sorted_g[0]\n",
        "        g_second_best_val = sorted_g[1][1] if len(sorted_g) > 1 else 0\n",
        "\n",
        "        avg_hb_hist = self._avg(self.hb_stat_values) if self.hb_stat_values else float('inf')\n",
        "        if not ((g_best_val - g_second_best_val >= epsilon) or (epsilon < avg_hb_hist)):\n",
        "            return False\n",
        "\n",
        "        impurity = calculate_gini_impurity(leaf.class_distribution)\n",
        "        if grow_fast: return True\n",
        "\n",
        "        current_impurities = [calculate_gini_impurity(n.class_distribution) for n in self.leaves if n.active]\n",
        "        c3 = impurity >= self._avg(current_impurities) - (self.std_dev_multiplier_c3_c4 * self._std_dev(current_impurities))\n",
        "        c4 = impurity >= self._avg(self.h_stat_values) - (self.std_dev_multiplier_c3_c4 * self._std_dev(self.h_stat_values))\n",
        "        c5 = g_best_val >= self._avg(self.g_stat_values) - (self.std_dev_multiplier_c5 * self._std_dev(self.g_stat_values))\n",
        "        c6 = leaf.n_l >= self._avg(self.n_stat_values)\n",
        "\n",
        "        if c3 and c4 and c5 and c6:\n",
        "            self._update_historical_stats(impurity, g_best_val, leaf.n_l, epsilon)\n",
        "            best_split_val, _ = leaf.feature_estimators[g_best_attr].get_best_split(impurity)\n",
        "            leaf.best_split_info = {'attribute': g_best_attr, 'split_value': best_split_val}\n",
        "            return True\n",
        "        return False\n",
        "\n",
        "    def _adapt_grace_period(self, leaf, g_values, epsilon):\n",
        "        sorted_g = sorted(g_values.values(), reverse=True)\n",
        "        g_best = sorted_g[0] if sorted_g else 0\n",
        "        g_second_best = sorted_g[1] if len(sorted_g) > 1 else 0\n",
        "        delta_g = g_best - g_second_best\n",
        "        avg_hb = self._avg(self.hb_stat_values) if self.hb_stat_values else epsilon\n",
        "\n",
        "        new_n_min = 0\n",
        "        if delta_g < epsilon and delta_g > avg_hb and delta_g > 1e-9:\n",
        "            new_n_min = math.ceil((self.R_HEURISTIC_RANGE**2 * math.log(1 / self.delta)) / (2 * delta_g**2))\n",
        "        elif delta_g < avg_hb and epsilon > avg_hb and avg_hb > 1e-9:\n",
        "            new_n_min = math.ceil((self.R_HEURISTIC_RANGE**2 * math.log(1 / self.delta)) / (2 * avg_hb**2))\n",
        "\n",
        "        if new_n_min > 0:\n",
        "            leaf.n_min_grace_period = max(leaf.n_min_grace_period, new_n_min)\n",
        "\n",
        "    def _calculate_g_values(self, leaf):\n",
        "        gains = {}\n",
        "        parent_impurity = calculate_gini_impurity(leaf.class_distribution)\n",
        "        for attr_index, splitter in leaf.feature_estimators.items():\n",
        "            _, gain = splitter.get_best_split(parent_impurity)\n",
        "            if gain > 0:\n",
        "                gains[attr_index] = gain\n",
        "        return gains\n",
        "\n",
        "    def _calculate_hoeffding_bound(self, n_instances):\n",
        "        if n_instances == 0: return float('inf')\n",
        "        return math.sqrt((self.R_HEURISTIC_RANGE**2 * math.log(1 / self.delta)) / (2 * n_instances))\n",
        "\n",
        "    def _update_historical_stats(self, impurity, g_best, n_l, epsilon):\n",
        "        self.h_stat_values.append(impurity)\n",
        "        self.g_stat_values.append(g_best)\n",
        "        self.n_stat_values.append(n_l)\n",
        "        self.hb_stat_values.append(epsilon)"
      ],
      "metadata": {
        "id": "MhRmG8F5jZKY"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "### **4. Utilitário de Carregamento de Dados**\n",
        "Esta célula contém uma função genérica para carregar um dataset a partir de um arquivo CSV local.\n",
        "\n",
        "* **O que é:** Um utilitário reutilizável para a preparação de dados.\n",
        "* **Como funciona:** Abre um arquivo CSV, ignora o cabeçalho e lê cada linha. Converte as `features` para `float` e as separa do rótulo da `class`. Retorna uma lista de tuplas, onde cada tupla contém um dicionário de instância e o seu rótulo correspondente, pronto para ser usado como um fluxo de dados.\n",
        "* **Parâmetros:**\n",
        "  * `file_path`: O caminho local para o arquivo CSV (ex: `'NOAA_weather.csv'`).\n",
        "  * `class_index`: O índice da coluna que contém o rótulo da `class`. O padrão é -1 (a última coluna)."
      ],
      "metadata": {
        "id": "5I4h5mfljGJ7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def load_csv_dataset(file_path, class_index=-1):\n",
        "    \"\"\"\n",
        "    A generic function to load a dataset from a local CSV file.\n",
        "    :param file_path: Path to the CSV file.\n",
        "    :param class_index: The index of the column containing the class label.\n",
        "    :return: A list of (instance_dict, label) tuples.\n",
        "    \"\"\"\n",
        "    print(f\"Loading dataset from local file: {file_path}...\")\n",
        "    stream = []\n",
        "    try:\n",
        "        with open(file_path, 'r', newline='') as f:\n",
        "            reader = csv.reader(f)\n",
        "            try:\n",
        "                # Attempt to skip header, but don't fail if it doesn't exist\n",
        "                header = next(reader)\n",
        "                print(f\"Dataset header: {header}\")\n",
        "            except StopIteration:\n",
        "                print(\"No header found or file is empty.\")\n",
        "                return []\n",
        "\n",
        "            for row in reader:\n",
        "                try:\n",
        "                    # Adjust class_index for feature list if it's negative\n",
        "                    if class_index < 0:\n",
        "                        effective_class_index = len(row) + class_index\n",
        "                    else:\n",
        "                        effective_class_index = class_index\n",
        "\n",
        "                    label = row[effective_class_index]\n",
        "\n",
        "                    features_list = [float(val) for i, val in enumerate(row) if i != effective_class_index]\n",
        "\n",
        "                    instance_X_dict = {j: features_list[j] for j in range(len(features_list))}\n",
        "                    stream.append((instance_X_dict, label))\n",
        "                except (ValueError, IndexError) as e:\n",
        "                    print(f\"Skipping malformed row: {row} | Error: {e}\")\n",
        "        print(f\"Successfully loaded {len(stream)} instances.\")\n",
        "        return stream\n",
        "    except FileNotFoundError:\n",
        "        print(f\"Error: File not found at '{file_path}'. Please upload it to your environment.\")\n",
        "        return None\n"
      ],
      "metadata": {
        "id": "R3HL06OojbDk"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "### **5. Execução e Configuração do Experimento**\n",
        "Este é o bloco de execução principal. Aqui, você configura o experimento escolhendo o dataset e definindo os hiperparâmetros do modelo.\n",
        "\n",
        "* **Como funciona:**\n",
        "  1. **Configuração:** Defina o `FILE_PATH` e o `CLASS_COLUMN_INDEX` para o dataset que você deseja testar.\n",
        "  2. **Carregar Dados:** Chame a função `load_csv_dataset` para preparar o fluxo de dados.\n",
        "  3. **Inicializar Modelo:** Crie uma instância da classe `DFDT`, passando hiperparâmetros como `delta` e `initial_grace_period`.\n",
        "  4. **Treinar:** Chame o método `train` na instância do modelo para iniciar o processo de aprendizagem."
      ],
      "metadata": {
        "id": "0m6Gjqpiihz-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def set_results_log(filepath, dataset_name, accuracy, exec_time, params=None):\n",
        "    if accuracy is not None and exec_time is not None:\n",
        "        result_line = f\"{dataset_name},{exec_time:.2f},{accuracy:.4f},{params if params else 'N/A'}\\n\"\n",
        "    else:\n",
        "        result_line = f\"{dataset_name},File Not Found,N/A,N/A\\n\"\n",
        "    write_header = not os.path.exists(filepath)\n",
        "    with open(filepath, 'a') as f:\n",
        "        if write_header:\n",
        "            f.write(\"dataset,executation_tie,accuracy,parameters\\n\")\n",
        "        f.write(result_line)\n",
        "\n",
        "def run_single_experiment(dataset_config, params):\n",
        "    file_path = dataset_config['path']\n",
        "    class_column_index = dataset_config['class_index']\n",
        "\n",
        "    if not os.path.exists(file_path):\n",
        "        print(f\"WARNING: File '{file_path}' not found.\")\n",
        "        return None, None\n",
        "\n",
        "    data_stream = load_csv_dataset(file_path, class_column_index)\n",
        "    if not data_stream:\n",
        "        return None, None\n",
        "\n",
        "    # Initialize model with all parameters from the grid\n",
        "    dfdt_model = DFDT(**params)\n",
        "\n",
        "    start_time = time.time()\n",
        "    final_accuracy = dfdt_model.train(data_stream)\n",
        "    end_time = time.time()\n",
        "    execution_time = end_time - start_time\n",
        "    return final_accuracy, execution_time\n",
        "\n",
        "def grid_search_experiment(dataset_config, param_grid, results_filepath):\n",
        "    best_accuracy = -1\n",
        "    best_params = {}\n",
        "\n",
        "    keys, values = zip(*param_grid.items())\n",
        "    param_combinations = [dict(zip(keys, v)) for v in itertools.product(*values)]\n",
        "\n",
        "    print(f\"\\n{'='*40}\")\n",
        "    print(f\"Starting Grid Search for: {dataset_config['name']}\")\n",
        "    print(f\"Testing {len(param_combinations)} parameter combinations.\")\n",
        "\n",
        "    for i, params in enumerate(param_combinations):\n",
        "        print(f\"  [Combination {i+1}/{len(param_combinations)}] Testing: {params}\")\n",
        "        accuracy, exec_time = run_single_experiment(dataset_config, params)\n",
        "        if accuracy is not None:\n",
        "            set_results_log(results_filepath, dataset_config['name'], accuracy, exec_time, params)\n",
        "            if accuracy > best_accuracy:\n",
        "                best_accuracy = accuracy\n",
        "                best_params = params\n",
        "\n",
        "    print(f\"--- Grid Search Complete for {dataset_config['name']} ---\")\n",
        "    print(f\"Best Accuracy: {best_accuracy:.4f}%\")\n",
        "    print(f\"Best Parameters: {best_params}\")"
      ],
      "metadata": {
        "id": "RTFwyhPSjabX"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "print(\"DFDT Python Implementation - Expanded Grid Search Runner\")\n",
        "print(\"=\" * 40)\n",
        "\n",
        "datasets = [\n",
        "    {'name': 'NOAA', 'path': 'NOAA.csv', 'class_index': 8},\n",
        "    {'name': 'Keystroke', 'path': 'Keystroke.csv', 'class_index': 10},\n",
        "    {'name': 'Chess', 'path': 'Chess.csv', 'class_index': 7},\n",
        "    {'name': 'Luxembourg', 'path': 'Luxembourg.csv', 'class_index': 31},\n",
        "    {'name': 'Ozone', 'path': 'Ozone.csv', 'class_index': 72},\n",
        "    {'name': 'SmartMeter', 'path': 'SmartMeter.csv', 'class_index': 96},\n",
        "    {'name': 'Electricity', 'path': 'Electricity.csv', 'class_index': 8},\n",
        "    {'name': 'Rialto', 'path': 'Rialto.csv', 'class_index': 27},\n",
        "    {'name': 'Forest', 'path': 'Forest.csv', 'class_index': 54},\n",
        "    {'name': 'Posture', 'path': 'Posture.csv', 'class_index': 3},\n",
        "    {'name': 'PokerHand', 'path': 'PokerHand.csv', 'class_index': 10},\n",
        "]\n",
        "# Nova grade de parâmetros expandida\n",
        "param_grid = {\n",
        "    'delta': [0.05],\n",
        "    'initial_grace_period': [200, 300, 400],\n",
        "    'grow_fast_threshold': [1.5],\n",
        "    'deactivate_threshold': [0.1],\n",
        "    'std_dev_multiplier_c3_c4': [0.5],\n",
        "    'std_dev_multiplier_c5': [0.5],\n",
        "    'r_heuristic_range': [0.45]\n",
        "}\n",
        "\n",
        "results_filepath = 'dfdt_expanded_grid_search_results2.csv'\n",
        "if os.path.exists(results_filepath):\n",
        "    os.remove(results_filepath)\n",
        "\n",
        "for config in datasets:\n",
        "    grid_search_experiment(config, param_grid, results_filepath)\n",
        "\n",
        "print(f\"\\n{'=' * 40}\")\n",
        "print(\"Processing complete!\")\n",
        "print(f\"All expanded grid search results saved to '{results_filepath}'.\")"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "DFDT Python Implementation - Expanded Grid Search Runner\n",
            "========================================\n",
            "\n",
            "========================================\n",
            "Starting Grid Search for: Forest\n",
            "Testing 3 parameter combinations.\n",
            "  [Combination 1/3] Testing: {'delta': 0.05, 'initial_grace_period': 200, 'grow_fast_threshold': 1.5, 'deactivate_threshold': 0.1, 'std_dev_multiplier_c3_c4': 0.5, 'std_dev_multiplier_c5': 0.5, 'r_heuristic_range': 0.45}\n",
            "Loading dataset from local file: Forest.csv...\n",
            "Dataset header: ['0.368684', '0.141667', '0.045455', '0.184681', '0.223514', '0.071659', '0.870079', '0.913386', '0.582677', '0.875366', '1', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '1', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '5']\n",
            "Successfully loaded 581011 instances.\n",
            "  [Combination 2/3] Testing: {'delta': 0.05, 'initial_grace_period': 300, 'grow_fast_threshold': 1.5, 'deactivate_threshold': 0.1, 'std_dev_multiplier_c3_c4': 0.5, 'std_dev_multiplier_c5': 0.5, 'r_heuristic_range': 0.45}\n",
            "Loading dataset from local file: Forest.csv...\n",
            "Dataset header: ['0.368684', '0.141667', '0.045455', '0.184681', '0.223514', '0.071659', '0.870079', '0.913386', '0.582677', '0.875366', '1', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '1', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '5']\n",
            "Successfully loaded 581011 instances.\n",
            "  [Combination 3/3] Testing: {'delta': 0.05, 'initial_grace_period': 400, 'grow_fast_threshold': 1.5, 'deactivate_threshold': 0.1, 'std_dev_multiplier_c3_c4': 0.5, 'std_dev_multiplier_c5': 0.5, 'r_heuristic_range': 0.45}\n",
            "Loading dataset from local file: Forest.csv...\n",
            "Dataset header: ['0.368684', '0.141667', '0.045455', '0.184681', '0.223514', '0.071659', '0.870079', '0.913386', '0.582677', '0.875366', '1', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '1', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '5']\n",
            "Successfully loaded 581011 instances.\n",
            "--- Grid Search Complete for Forest ---\n",
            "Best Accuracy: 63.1084%\n",
            "Best Parameters: {'delta': 0.05, 'initial_grace_period': 400, 'grow_fast_threshold': 1.5, 'deactivate_threshold': 0.1, 'std_dev_multiplier_c3_c4': 0.5, 'std_dev_multiplier_c5': 0.5, 'r_heuristic_range': 0.45}\n",
            "\n",
            "========================================\n",
            "Processing complete!\n",
            "All expanded grid search results saved to 'dfdt_expanded_grid_search_results2.csv'.\n"
          ]
        }
      ],
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vRQLrXnxifF6",
        "outputId": "590f9aee-2a64-45aa-ae91-a712f79589f3"
      }
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}